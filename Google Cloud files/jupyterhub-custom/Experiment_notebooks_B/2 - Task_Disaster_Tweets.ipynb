{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3002ffa1",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"Please carefully follow these instructions:\\n\\nPlease open the graph view by clicking on the icon <img src=\\\"img/graph.svg\\\" width=\\\"15\\\"> in the menu on the left.\\n\\nThis notebook includes additional interface features that support exploratory programming:\\n\\n- Switching between alternative code versions\\n\\n- Collapsing and zooming into code sections\\n\\n- Graph-based navigation\\n\\nYou are encouraged to use all available features to help you understand and modify the notebook.\\n\\nOnly follow the exercises under markdown cells that start with Task. These are the parts where you are expected to do something (e.g. debug or modify code).\\n\\nPlease try to complete the tasks to the best of your ability, but don’t worry if you don’t know everything.\\n\\nOnce you're done, proceed to the next notebook: 3 - Post-survey.\\n\\nGood luck!\"}],\"activeIndex\":0}"
   },
   "source": [
    "Please carefully follow these instructions:\n",
    "\n",
    "Please open the graph view by clicking on the icon <img src=\"img/graph.svg\" width=\"15\"> in the menu on the left.\n",
    "\n",
    "This notebook includes additional interface features that support exploratory programming:\n",
    "\n",
    "- Switching between alternative code versions\n",
    "\n",
    "- Collapsing and zooming into code sections\n",
    "\n",
    "- Graph-based navigation\n",
    "\n",
    "You are encouraged to use all available features to help you understand and modify the notebook.\n",
    "\n",
    "Only follow the exercises under markdown cells that start with Task. These are the parts where you are expected to do something (e.g. debug or modify code).\n",
    "\n",
    "Please try to complete the tasks to the best of your ability, but don’t worry if you don’t know everything.\n",
    "\n",
    "Once you're done, proceed to the next notebook: 3 - Post-survey.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dadf72",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"#Please execute this code cell to download the stopwords\\nimport nltk\\nnltk.download('stopwords')\"}],\"activeIndex\":0}"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please execute this code cell to download the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d845e9-2617-479e-9b3f-74af141f62f8",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"# Exploring Tweet Cleaning and Notebook Structure\\n\\nThis notebook uses a dataset of tweets labeled as either:\\n- **1**: The tweet describes a real disaster\\n- **0**: The tweet does not\\n\\nThe aim is to prepare these tweets for machine learning by cleaning the text and removing unnecessary tokens like stopwords.\\n\\nA previous analyst worked on this notebook and tried several approaches. Your job is to explore what has already been done.\\n\\nYou will encounter:\\n- Multiple versions of similar code\\n- Possibly unused or inconsistent cells\\n- Potential issues that require debugging\\n\\nFocus on understanding the structure, not just running code.\\n\"}],\"activeIndex\":0}",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring Tweet Cleaning and Notebook Structure\n",
    "\n",
    "This notebook uses a dataset of tweets labeled as either:\n",
    "- **1**: The tweet describes a real disaster\n",
    "- **0**: The tweet does not\n",
    "\n",
    "The aim is to prepare these tweets for machine learning by cleaning the text and removing unnecessary tokens like stopwords.\n",
    "\n",
    "A previous analyst worked on this notebook and tried several approaches. Your job is to explore what has already been done.\n",
    "\n",
    "You will encounter:\n",
    "- Multiple versions of similar code\n",
    "- Possibly unused or inconsistent cells\n",
    "- Potential issues that require debugging\n",
    "\n",
    "Focus on understanding the structure, not just running code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420ecbf-e01b-429b-accd-5fbb2c79acfa",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"# Part 1 – Cleaning Text and Understanding the Notebook\\n\\nIn this section, you'll explore different approaches the analyst used to clean tweet text.\\n\\nYour tasks:\\n1. Understand what each cleaning function is doing\\n2. Determine which version(s) were actually used later\\n3. Identify code that was defined but never used\\n4. Pay attention to the data flow, some mistakes may be subtle\\n\"}],\"activeIndex\":0}"
   },
   "source": [
    "# Part 1 – Cleaning Text and Understanding the Notebook\n",
    "\n",
    "In this section, you'll explore different approaches the analyst used to clean tweet text.\n",
    "\n",
    "Your tasks:\n",
    "1. Understand what each cleaning function is doing\n",
    "2. Determine which version(s) were actually used later\n",
    "3. Identify code that was defined but never used\n",
    "4. Pay attention to the data flow, some mistakes may be subtle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9989306d-3b3b-48ce-a207-181e325fb30d",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"import pandas as pd\\n\\n# Load dataset\\ntrain = pd.read_csv(\\\"data/tweets.csv\\\")\\ntrain.head()\"}],\"activeIndex\":0}"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "train = pd.read_csv(\"data/tweets.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9718c9",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"# Show some tweets\\ntrain[['text', 'target']].sample(10, random_state=42)\"}],\"activeIndex\":0}"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>Help yourself or those you love who suffer fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>@BLutz10 But the rioting began prior to the de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>If the Taken movies took place in India 2 (Vin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>Longest Streak of Triple-Digit Heat Since 2013...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>I want some tsunami take out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>New and now: Different (FNaF fanfiction): Trix...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6213</th>\n",
       "      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>Hail Mary Full of Grace The Lord is with thee....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7488</th>\n",
       "      <td>act my age was a MESS everyone was so wild it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "683   Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...       0\n",
       "7444  Help yourself or those you love who suffer fro...       0\n",
       "5803  @BLutz10 But the rioting began prior to the de...       1\n",
       "2484  If the Taken movies took place in India 2 (Vin...       0\n",
       "4279  Longest Streak of Triple-Digit Heat Since 2013...       1\n",
       "6973                       I want some tsunami take out       0\n",
       "2929  New and now: Different (FNaF fanfiction): Trix...       0\n",
       "6213  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0\n",
       "4086  Hail Mary Full of Grace The Lord is with thee....       0\n",
       "7488  act my age was a MESS everyone was so wild it ...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some tweets\n",
    "train[['text', 'target']].sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a2792f-d403-4505-9cab-41691630c52e",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"import re\\n\\nfrom nltk.corpus import stopwords\\nstop_words = set(stopwords.words('english'))\"}],\"activeIndex\":0}"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "a1",
   "id": "500effcf",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"\\\"\\\"\\\"\\nVersion 1 – Full Stopword Removal (clean_text_v1)\\n\\nThis version removes **all** standard English stopwords using `nltk`, including pronouns like \\\"I\\\", \\\"we\\\", and \\\"they\\\".\\n\\nIt may be too aggressive for tweets, which often rely on personal language.\\n\\\"\\\"\\\"\\n\\ndef clean_text_v1(text):\\n    if pd.isnull(text):\\n        return \\\"\\\"\\n    text = re.sub(r\\\"http\\\\S+\\\", \\\"\\\", text)\\n    text = re.sub(r\\\"[^a-zA-Z\\\\s]\\\", \\\"\\\", text)\\n    words = text.lower().split()\\n    words = [word for word in words if word not in stop_words]\\n    return \\\" \\\".join(words)\\n\\ntrain['cleaned_v1'] = train['text'].apply(clean_text_v1)\\n\"},{\"source\":\"\\\"\\\"\\\"\\nVersion 2 – Alternative Cleaning Strategy (clean_text_v2)\\n\\nThis version appears to modify the stopword list to preserve certain words.\\n\\nCompare the output to other versions and consider:\\n- What is this version trying to do?\\n- Does it behave as expected?\\n\\\"\\\"\\\"\\n\\npronouns = {'i', 'you', 'we', 'they', 'he', 'she', 'me', 'us', 'them'}\\n\\ndef clean_text_v2(text):\\n    if pd.isnull(text):\\n        return \\\"\\\"\\n\\n    text = re.sub(r\\\"http\\\\S+\\\", \\\"\\\", text)\\n    text = re.sub(r\\\"[^a-zA-Z\\\\s]\\\", \\\"\\\", text)\\n    words = text.lower().split()\\n\\n    # Tries to define a filtered stopword list\\n    custom_stopwords = stop_words.intersection(pronouns)\\n\\n    # Apply stopword filter (but this list now includes only pronouns)\\n    words = [word for word in words if word not in custom_stopwords]\\n\\n    return \\\" \\\".join(words)\\n\\ntrain['cleaned_v2'] = train['text'].apply(clean_text_v2)\\n\"},{\"source\":\"\\\"\\\"\\\"\\nVersion 3 – Final Fix Attempt (clean_text_v3)\\n\\nThis version fixes the stopword list by removing pronouns from it before filtering.\\n\\nThis should preserve important personal words like \\\"I\\\", \\\"we\\\", and \\\"you\\\".\\n\\\"\\\"\\\"\\n\\nfiltered_stopwords = stop_words - pronouns\\n\\ndef clean_text_v3(text):\\n    if pd.isnull(text):\\n        return \\\"\\\"\\n    text = re.sub(r\\\"http\\\\S+\\\", \\\"\\\", text)\\n    text = re.sub(r\\\"[^a-zA-Z\\\\s]\\\", \\\"\\\", text)\\n    words = text.lower().split()\\n    words = [word for word in words if word not in filtered_stopwords]\\n    return \\\" \\\".join(words)\\n\\ntrain['cleaned_v3'] = train['text'].apply(clean_text_v3)\\n\"}],\"activeIndex\":1}"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Version 2 – Alternative Cleaning Strategy (clean_text_v2)\n",
    "\n",
    "This version appears to modify the stopword list to preserve certain words.\n",
    "\n",
    "Compare the output to other versions and consider:\n",
    "- What is this version trying to do?\n",
    "- Does it behave as expected?\n",
    "\"\"\"\n",
    "\n",
    "pronouns = {'i', 'you', 'we', 'they', 'he', 'she', 'me', 'us', 'them'}\n",
    "\n",
    "def clean_text_v2(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Tries to define a filtered stopword list\n",
    "    custom_stopwords = stop_words.intersection(pronouns)\n",
    "\n",
    "    # Apply stopword filter (but this list now includes only pronouns)\n",
    "    words = [word for word in words if word not in custom_stopwords]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "train['cleaned_v2'] = train['text'].apply(clean_text_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9d39e2-e8a7-4982-b1c0-ea7132810586",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"train[['text', 'cleaned_v1', 'cleaned_v2', 'cleaned_v3']].sample(10, random_state=42)\\n\"}],\"activeIndex\":0}"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_v1</th>\n",
       "      <th>cleaned_v2</th>\n",
       "      <th>cleaned_v3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...</td>\n",
       "      <td>morgan silver dollar gem bu dmpl cameo rev bla...</td>\n",
       "      <td>morgan silver dollar s gem bu dmpl cameo rev b...</td>\n",
       "      <td>morgan silver dollar gem bu dmpl cameo rev bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>Help yourself or those you love who suffer fro...</td>\n",
       "      <td>help love suffer selfesteem wounds today</td>\n",
       "      <td>help yourself or those love who suffer from se...</td>\n",
       "      <td>help you love suffer selfesteem wounds you today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>@BLutz10 But the rioting began prior to the de...</td>\n",
       "      <td>blutz rioting began prior decision indictment ...</td>\n",
       "      <td>blutz but the rioting began prior to the decis...</td>\n",
       "      <td>blutz rioting began prior decision indictment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>If the Taken movies took place in India 2 (Vin...</td>\n",
       "      <td>taken movies took place india vine jusreign</td>\n",
       "      <td>if the taken movies took place in india vine b...</td>\n",
       "      <td>taken movies took place india vine jusreign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>Longest Streak of Triple-Digit Heat Since 2013...</td>\n",
       "      <td>longest streak tripledigit heat since forecast...</td>\n",
       "      <td>longest streak of tripledigit heat since forec...</td>\n",
       "      <td>longest streak tripledigit heat since forecast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>I want some tsunami take out</td>\n",
       "      <td>want tsunami take</td>\n",
       "      <td>want some tsunami take out</td>\n",
       "      <td>i want tsunami take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>New and now: Different (FNaF fanfiction): Trix...</td>\n",
       "      <td>new different fnaf fanfiction trixiedrowned pa...</td>\n",
       "      <td>new and now different fnaf fanfiction trixiedr...</td>\n",
       "      <td>new different fnaf fanfiction trixiedrowned pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6213</th>\n",
       "      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n",
       "      <td>lionel trains smoke locomotives magnetraction ...</td>\n",
       "      <td>lionel trains smoke locomotives with magnetrac...</td>\n",
       "      <td>lionel trains smoke locomotives magnetraction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>Hail Mary Full of Grace The Lord is with thee....</td>\n",
       "      <td>hail mary full grace lord thee blessed art tho...</td>\n",
       "      <td>hail mary full of grace the lord is with thee ...</td>\n",
       "      <td>hail mary full grace lord thee blessed art tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7488</th>\n",
       "      <td>act my age was a MESS everyone was so wild it ...</td>\n",
       "      <td>act age mess everyone wild fun videos wreck</td>\n",
       "      <td>act my age was a mess everyone was so wild it ...</td>\n",
       "      <td>act age mess everyone wild fun videos wreck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "683   Morgan Silver Dollar 1880 S Gem BU DMPL Cameo ...   \n",
       "7444  Help yourself or those you love who suffer fro...   \n",
       "5803  @BLutz10 But the rioting began prior to the de...   \n",
       "2484  If the Taken movies took place in India 2 (Vin...   \n",
       "4279  Longest Streak of Triple-Digit Heat Since 2013...   \n",
       "6973                       I want some tsunami take out   \n",
       "2929  New and now: Different (FNaF fanfiction): Trix...   \n",
       "6213  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...   \n",
       "4086  Hail Mary Full of Grace The Lord is with thee....   \n",
       "7488  act my age was a MESS everyone was so wild it ...   \n",
       "\n",
       "                                             cleaned_v1  \\\n",
       "683   morgan silver dollar gem bu dmpl cameo rev bla...   \n",
       "7444           help love suffer selfesteem wounds today   \n",
       "5803  blutz rioting began prior decision indictment ...   \n",
       "2484        taken movies took place india vine jusreign   \n",
       "4279  longest streak tripledigit heat since forecast...   \n",
       "6973                                  want tsunami take   \n",
       "2929  new different fnaf fanfiction trixiedrowned pa...   \n",
       "6213  lionel trains smoke locomotives magnetraction ...   \n",
       "4086  hail mary full grace lord thee blessed art tho...   \n",
       "7488        act age mess everyone wild fun videos wreck   \n",
       "\n",
       "                                             cleaned_v2  \\\n",
       "683   morgan silver dollar s gem bu dmpl cameo rev b...   \n",
       "7444  help yourself or those love who suffer from se...   \n",
       "5803  blutz but the rioting began prior to the decis...   \n",
       "2484  if the taken movies took place in india vine b...   \n",
       "4279  longest streak of tripledigit heat since forec...   \n",
       "6973                         want some tsunami take out   \n",
       "2929  new and now different fnaf fanfiction trixiedr...   \n",
       "6213  lionel trains smoke locomotives with magnetrac...   \n",
       "4086  hail mary full of grace the lord is with thee ...   \n",
       "7488  act my age was a mess everyone was so wild it ...   \n",
       "\n",
       "                                             cleaned_v3  \n",
       "683   morgan silver dollar gem bu dmpl cameo rev bla...  \n",
       "7444   help you love suffer selfesteem wounds you today  \n",
       "5803  blutz rioting began prior decision indictment ...  \n",
       "2484        taken movies took place india vine jusreign  \n",
       "4279  longest streak tripledigit heat since forecast...  \n",
       "6973                                i want tsunami take  \n",
       "2929  new different fnaf fanfiction trixiedrowned pa...  \n",
       "6213  lionel trains smoke locomotives magnetraction ...  \n",
       "4086  hail mary full grace lord thee blessed art tho...  \n",
       "7488        act age mess everyone wild fun videos wreck  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['text', 'cleaned_v1', 'cleaned_v2', 'cleaned_v3']].sample(10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e11f25-3212-438c-b224-66c5ec922393",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"## Task 1 – Compare Cleaning Functions\\n\\nFill out the following table based on what you understand from the code above:\\n\\n| Version        | Stopwords Removed | Pronouns Kept | Cleaned Column | Notes or Issues                    |\\n|----------------|-------------------|----------------|----------------|-------------------------------------|\\n| clean_text_v1  |                   |                | cleaned_v1     |                                     |\\n| clean_text_v2  |                   |                | cleaned_v2     |                                     |\\n| clean_text_v3  |                   |                | cleaned_v3     |                                     |\\n\\nTip: Don't guess from output — read the code carefully.\\n\"}],\"activeIndex\":0}"
   },
   "source": [
    "## Task 1 – Compare Cleaning Functions\n",
    "\n",
    "Fill out the following table based on what you understand from the code above:\n",
    "\n",
    "| Version        | Stopwords Removed | Pronouns Kept | Cleaned Column | Notes or Issues                    |\n",
    "|----------------|-------------------|----------------|----------------|-------------------------------------|\n",
    "| clean_text_v1  |                   |                | cleaned_v1     |                                     |\n",
    "| clean_text_v2  |                   |                | cleaned_v2     |                                     |\n",
    "| clean_text_v3  |                   |                | cleaned_v3     |                                     |\n",
    "\n",
    "Tip: Don't guess from output — read the code carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc67fb-3b25-42f7-8bfa-4f29c0b2f315",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"## Part 2: Vectorizing the Cleaned Text\\n\\nThe analyst began converting the cleaned tweets into numerical features using `TfidfVectorizer`. This step is common in text processing, as it turns words into numbers based on how important they are.\\n\\nThey attempted this in a few different ways. However, the result either caused errors or didn’t behave as expected later in the notebook.\\n\\nYour task is to review these steps and identify what needs to be adjusted.\\n\\n\"}],\"activeIndex\":0}"
   },
   "source": [
    "## Part 2: Vectorizing the Cleaned Text\n",
    "\n",
    "The analyst began converting the cleaned tweets into numerical features using `TfidfVectorizer`. This step is common in text processing, as it turns words into numbers based on how important they are.\n",
    "\n",
    "They attempted this in a few different ways. However, the result either caused errors or didn’t behave as expected later in the notebook.\n",
    "\n",
    "Your task is to review these steps and identify what needs to be adjusted.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97402ba6-93b9-4c69-8dd6-4fdff87f5a66",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"## Task 3: Fix the Vectorization Pipeline\\n\\nReview the steps taken to vectorize the cleaned tweet data.\\n\\nYour goal is to make sure the vectorized output:\\n- Uses the correct cleaned text column\\n- Does not include rows where the cleaned text is empty\\n- Stays correctly aligned with the original dataset\\n\\nYou may want to check the number of rows in the vectorized output compared to the number of rows in the cleaned DataFrame.\\n\\nName your version `X_final`.\\n\"}],\"activeIndex\":0}"
   },
   "source": [
    "## Task 2: Fix the Vectorization Pipeline\n",
    "\n",
    "Review the steps taken to vectorize the cleaned tweet data.\n",
    "\n",
    "Your goal is to make sure the vectorized output:\n",
    "- Uses the correct cleaned text column\n",
    "- Does not include rows where the cleaned text is empty\n",
    "- Stays correctly aligned with the original dataset\n",
    "\n",
    "You may want to check the number of rows in the vectorized output compared to the number of rows in the cleaned DataFrame.\n",
    "\n",
    "Name your version `X_final`.\n",
    "\n",
    "To enter the raffle, please give \"Enter\" as an answer to \"Questions?\" after entering your email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f9828-e3eb-406f-aa3e-41260eabec56",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"from sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvectorizer = TfidfVectorizer()\\n\\n# Bug: applies TF-IDF to uncleaned text instead of cleaned_v3\\nX_wrong = vectorizer.fit_transform(train['text'])\\n\\n# This includes raw punctuation, casing, etc.\\nprint(X_wrong.shape)\\n\\n\\nvectorizer = TfidfVectorizer()\\n\\n# Applies to cleaned_v3, but dataset still contains empty rows\\nX_partial = vectorizer.fit_transform(train['cleaned_v3'])\\n\\n# May include empty strings\\nprint(train['cleaned_v3'].iloc[-25:-15])\\n\\n\\n# Applies vectorization first, THEN removes empty strings\\nX_mismatch = vectorizer.fit_transform(train['cleaned_v3'])\\n\\ntrain = train[train['cleaned_v3'].str.strip() != \\\"\\\"]\\n\\n#Mismatch between shape and correct shape\\nprint(X_mismatch.shape, train.shape[0])\\n\\n\\n# Please write your solution underneath\\n\\nX_final = ...\\n\\n# Confirm shapes match\\nprint(\\\"Shape of feature matrix:\\\", X_final.shape)\\nprint(\\\"Number of rows in DataFrame:\\\", train.shape[0])\"}],\"activeIndex\":0}",
    "collapsed-data": "{\"storedNodes\":[{\"cellId\":\"63a20f14-8f5a-48f1-8a81-4276ae7ea8b8\",\"source\":\"from sklearn.feature_extraction.text import TfidfVectorizer\\n\\nvectorizer = TfidfVectorizer()\\n\\n# Bug: applies TF-IDF to uncleaned text instead of cleaned_v3\\nX_wrong = vectorizer.fit_transform(train['text'])\\n\\n# This includes raw punctuation, casing, etc.\\nprint(X_wrong.shape)\\n\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"from sklearn.feature_extraction.text import TfidfVectorizer\\\\n\\\\nvectorizer = TfidfVectorizer()\\\\n\\\\n# Bug: applies TF-IDF to uncleaned text instead of cleaned_v3\\\\nX_wrong = vectorizer.fit_transform(train['text'])\\\\n\\\\n# This includes raw punctuation, casing, etc.\\\\nprint(X_wrong.shape)\\\\n\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"dc5e8a81-1efc-4ed7-bad0-6621a94a0155\",\"source\":\"vectorizer = TfidfVectorizer()\\n\\n# Applies to cleaned_v3, but dataset still contains empty rows\\nX_partial = vectorizer.fit_transform(train['cleaned_v3'])\\n\\n# May include empty strings\\nprint(train['cleaned_v3'].iloc[-25:-15])\\n\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"vectorizer = TfidfVectorizer()\\\\n\\\\n# Applies to cleaned_v3, but dataset still contains empty rows\\\\nX_partial = vectorizer.fit_transform(train['cleaned_v3'])\\\\n\\\\n# May include empty strings\\\\nprint(train['cleaned_v3'].iloc[-25:-15])\\\\n\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"4dc4be99-5010-43f6-8556-34ab16131d41\",\"source\":\"# Applies vectorization first, THEN removes empty strings\\nX_mismatch = vectorizer.fit_transform(train['cleaned_v3'])\\n\\ntrain = train[train['cleaned_v3'].str.strip() != \\\"\\\"]\\n\\n#Mismatch between shape and correct shape\\nprint(X_mismatch.shape, train.shape[0])\\n\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"# Applies vectorization first, THEN removes empty strings\\\\nX_mismatch = vectorizer.fit_transform(train['cleaned_v3'])\\\\n\\\\ntrain = train[train['cleaned_v3'].str.strip() != \\\\\\\"\\\\\\\"]\\\\n\\\\n#Mismatch between shape and correct shape\\\\nprint(X_mismatch.shape, train.shape[0])\\\\n\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"8e301735-dc00-4611-9273-d8ed6964febd\",\"source\":\"# Please write your solution underneath\\n\\nX_final = ...\\n\\n# Confirm shapes match\\nprint(\\\"Shape of feature matrix:\\\", X_final.shape)\\nprint(\\\"Number of rows in DataFrame:\\\", train.shape[0])\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"# Please write your solution underneath\\\\n\\\\nX_final = ...\\\\n\\\\n# Confirm shapes match\\\\nprint(\\\\\\\"Shape of feature matrix:\\\\\\\", X_final.shape)\\\\nprint(\\\\\\\"Number of rows in DataFrame:\\\\\\\", train.shape[0])\\\"}],\\\"activeIndex\\\":0}\"}],\"notebookName\":\"temp-notebook-6b16bdfa-9caf-47ec-87ab-5b5d025f852f.ipynb\"}",
    "readOnly": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Bug: applies TF-IDF to uncleaned text instead of cleaned_v3\n",
    "X_wrong = vectorizer.fit_transform(train['text'])\n",
    "\n",
    "# This includes raw punctuation, casing, etc.\n",
    "print(X_wrong.shape)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Applies to cleaned_v3, but dataset still contains empty rows\n",
    "X_partial = vectorizer.fit_transform(train['cleaned_v3'])\n",
    "\n",
    "# May include empty strings\n",
    "print(train['cleaned_v3'].iloc[-25:-15])\n",
    "\n",
    "\n",
    "# Applies vectorization first, THEN removes empty strings\n",
    "X_mismatch = vectorizer.fit_transform(train['cleaned_v3'])\n",
    "\n",
    "train = train[train['cleaned_v3'].str.strip() != \"\"]\n",
    "\n",
    "#Mismatch between shape and correct shape\n",
    "print(X_mismatch.shape, train.shape[0])\n",
    "\n",
    "\n",
    "# Please write your solution underneath\n",
    "\n",
    "X_final = ...\n",
    "\n",
    "# Confirm shapes match\n",
    "print(\"Shape of feature matrix:\", X_final.shape)\n",
    "print(\"Number of rows in DataFrame:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916dad0e-6389-47c1-950d-ce7eb67bf7a6",
   "metadata": {},
   "source": [
    "# Part 3 – Evaluating the Classifier\n",
    "\n",
    "In this section, the analyst tried to evaluate the performance of a simple classifier on the tweet dataset.\n",
    "\n",
    "However, the evaluation result does not seem correct. You might notice that:\n",
    "- The accuracy is unusually high or low\n",
    "- The code runs without error, but the numbers don’t make sense\n",
    "- The prediction or evaluation is based on mismatched data\n",
    "\n",
    "\n",
    "## Task 3 – Debug the Evaluation\n",
    "\n",
    "Your goal is to fix the evaluation process so that it correctly shows how well the classifier performs.\n",
    "\n",
    "Look out for:\n",
    "- Whether training and testing data are correctly split\n",
    "- Whether predictions are made on the correct data\n",
    "- Whether accuracy is being calculated against the right labels\n",
    "\n",
    "Make sure the final accuracy score reflects actual model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabd103-dc04-4620-ae44-7b2bdc0d1de7",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"from sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\n\\n# Fit model\\nclf = LogisticRegression()\\nclf.fit(X_final, train['target'])\\n\\n# Make predictions\\npreds = clf.predict(X_final)\\n\\n# Compare to test labels\\nprint(\\\"Accuracy:\\\", accuracy_score(test['target'], preds))\"}],\"activeIndex\":0}",
    "collapsed-data": "{\"storedNodes\":[{\"cellId\":\"5031694a-3c4c-4f84-843f-88d2c18c11dc\",\"source\":\"from sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"from sklearn.model_selection import train_test_split\\\\nfrom sklearn.linear_model import LogisticRegression\\\\nfrom sklearn.metrics import accuracy_score\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"d9708d9c-6a3a-4ba1-b4a3-ff5f424c6dd1\",\"source\":\"# Fit model\\nclf = LogisticRegression()\\nclf.fit(X_final, train['target'])\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"# Fit model\\\\nclf = LogisticRegression()\\\\nclf.fit(X_final, train['target'])\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"60c7a45e-f0cc-46b6-80e6-625b9e3cb055\",\"source\":\"# Make predictions\\npreds = clf.predict(X_final)\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"# Make predictions\\\\npreds = clf.predict(X_final)\\\"}],\\\"activeIndex\\\":0}\"},{\"cellId\":\"eef7b408-082b-4f80-aa73-2cf5d920e8e4\",\"source\":\"# Compare to test labels\\nprint(\\\"Accuracy:\\\", accuracy_score(test['target'], preds))\",\"alternativeMetadata\":\"{\\\"versions\\\":[{\\\"source\\\":\\\"# Compare to test labels\\\\nprint(\\\\\\\"Accuracy:\\\\\\\", accuracy_score(test['target'], preds))\\\"}],\\\"activeIndex\\\":0}\"}],\"notebookName\":\"temp-notebook-26dd7266-c286-468e-a7c4-95573f129880.ipynb\"}",
    "readOnly": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_final, train['target'])\n",
    "\n",
    "# Make predictions\n",
    "preds = clf.predict(X_final)\n",
    "\n",
    "# Compare to test labels\n",
    "print(\"Accuracy:\", accuracy_score(test['target'], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c00e1b-c336-499d-9565-82efd3b6ee50",
   "metadata": {
    "alternatives-data": "{\"versions\":[{\"source\":\"After finishing this tasou may now go to the Post-survey.\"}],\"activeIndex\":0}"
   },
   "source": [
    "After finishing this task, you may now go to the Post-survey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
